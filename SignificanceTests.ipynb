{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FewvajNuWUDu"
   },
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrK1zZSeJ9TD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "import numpy.matlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezn6xVnYWaGn"
   },
   "source": [
    "# Data Generation:\n",
    "\n",
    "We generate samples X that are uniformly distributed on [-1,1]^8 and then compupte the output Y of our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "660OZTpHPQJk",
    "outputId": "5eef5746-626d-4be7-d116-ef2f6f1c72d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.165956</td>\n",
       "      <td>0.440649</td>\n",
       "      <td>-0.999771</td>\n",
       "      <td>-0.395335</td>\n",
       "      <td>-0.706488</td>\n",
       "      <td>-0.815323</td>\n",
       "      <td>-0.627480</td>\n",
       "      <td>-0.308879</td>\n",
       "      <td>10.218395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.206465</td>\n",
       "      <td>0.077633</td>\n",
       "      <td>-0.161611</td>\n",
       "      <td>0.370439</td>\n",
       "      <td>-0.591096</td>\n",
       "      <td>0.756235</td>\n",
       "      <td>-0.945225</td>\n",
       "      <td>0.340935</td>\n",
       "      <td>9.505227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.165390</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>-0.719226</td>\n",
       "      <td>-0.603797</td>\n",
       "      <td>0.601489</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>-0.373152</td>\n",
       "      <td>0.384645</td>\n",
       "      <td>10.492027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.752778</td>\n",
       "      <td>0.789213</td>\n",
       "      <td>-0.829912</td>\n",
       "      <td>-0.921890</td>\n",
       "      <td>-0.660339</td>\n",
       "      <td>0.756285</td>\n",
       "      <td>-0.803306</td>\n",
       "      <td>-0.157785</td>\n",
       "      <td>9.026440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.915779</td>\n",
       "      <td>0.066331</td>\n",
       "      <td>0.383754</td>\n",
       "      <td>-0.368969</td>\n",
       "      <td>0.373002</td>\n",
       "      <td>0.669251</td>\n",
       "      <td>-0.963423</td>\n",
       "      <td>0.500289</td>\n",
       "      <td>10.966825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101995</th>\n",
       "      <td>-0.965163</td>\n",
       "      <td>-0.868066</td>\n",
       "      <td>0.687381</td>\n",
       "      <td>0.737635</td>\n",
       "      <td>0.201860</td>\n",
       "      <td>0.130245</td>\n",
       "      <td>-0.953401</td>\n",
       "      <td>0.992092</td>\n",
       "      <td>10.001159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101996</th>\n",
       "      <td>0.801107</td>\n",
       "      <td>0.875722</td>\n",
       "      <td>-0.990787</td>\n",
       "      <td>0.911076</td>\n",
       "      <td>0.353634</td>\n",
       "      <td>-0.470982</td>\n",
       "      <td>-0.985444</td>\n",
       "      <td>-0.509583</td>\n",
       "      <td>9.140565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101997</th>\n",
       "      <td>0.126903</td>\n",
       "      <td>-0.399359</td>\n",
       "      <td>-0.606135</td>\n",
       "      <td>-0.742466</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.924128</td>\n",
       "      <td>-0.592043</td>\n",
       "      <td>0.104867</td>\n",
       "      <td>9.935730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101998</th>\n",
       "      <td>-0.102098</td>\n",
       "      <td>-0.232576</td>\n",
       "      <td>-0.926793</td>\n",
       "      <td>-0.189997</td>\n",
       "      <td>-0.359941</td>\n",
       "      <td>-0.060435</td>\n",
       "      <td>-0.958450</td>\n",
       "      <td>0.175319</td>\n",
       "      <td>10.154265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101999</th>\n",
       "      <td>-0.393309</td>\n",
       "      <td>0.957055</td>\n",
       "      <td>-0.376406</td>\n",
       "      <td>-0.531251</td>\n",
       "      <td>-0.746653</td>\n",
       "      <td>-0.542937</td>\n",
       "      <td>0.639591</td>\n",
       "      <td>0.487487</td>\n",
       "      <td>10.207424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X_1       X_2       X_3       X_4       X_5       X_6       X_7  \\\n",
       "0      -0.165956  0.440649 -0.999771 -0.395335 -0.706488 -0.815323 -0.627480   \n",
       "1      -0.206465  0.077633 -0.161611  0.370439 -0.591096  0.756235 -0.945225   \n",
       "2      -0.165390  0.117380 -0.719226 -0.603797  0.601489  0.936523 -0.373152   \n",
       "3       0.752778  0.789213 -0.829912 -0.921890 -0.660339  0.756285 -0.803306   \n",
       "4       0.915779  0.066331  0.383754 -0.368969  0.373002  0.669251 -0.963423   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "101995 -0.965163 -0.868066  0.687381  0.737635  0.201860  0.130245 -0.953401   \n",
       "101996  0.801107  0.875722 -0.990787  0.911076  0.353634 -0.470982 -0.985444   \n",
       "101997  0.126903 -0.399359 -0.606135 -0.742466 -0.001000 -0.924128 -0.592043   \n",
       "101998 -0.102098 -0.232576 -0.926793 -0.189997 -0.359941 -0.060435 -0.958450   \n",
       "101999 -0.393309  0.957055 -0.376406 -0.531251 -0.746653 -0.542937  0.639591   \n",
       "\n",
       "             X_8          Y  \n",
       "0      -0.308879  10.218395  \n",
       "1       0.340935   9.505227  \n",
       "2       0.384645  10.492027  \n",
       "3      -0.157785   9.026440  \n",
       "4       0.500289  10.966825  \n",
       "...          ...        ...  \n",
       "101995  0.992092  10.001159  \n",
       "101996 -0.509583   9.140565  \n",
       "101997  0.104867   9.935730  \n",
       "101998  0.175319  10.154265  \n",
       "101999  0.487487  10.207424  \n",
       "\n",
       "[102000 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000 # Number of samples\n",
    "m = 1000 # Number of samples for validation/testing dataset\n",
    "d = 8 # Dimension of feature variable vector\n",
    "sigma = 0.01\n",
    "\n",
    "np.random.seed(1) # Set seed for reproducibility\n",
    "X = np.random.uniform(-1,1,(n + 2*m,d)) # 8 dimensional random vector X uniformly distributed on [-1,1]\n",
    "eps = np.random.normal(0,sigma,n + 2*m) # Gaussian noise\n",
    "\n",
    "Y = 8 + X[:,0]**2 + X[:,1]*X[:,2] + np.cos(X[:,3]) + np.exp(X[:,4]*X[:,5]) + 0.1*X[:,6] + eps # Output computation\n",
    "Y = Y.reshape((-1,1)) # Transpose output vector\n",
    "\n",
    "data = np.concatenate((X,Y), axis=1) # First 8 columns as input and last column as output\n",
    "data_df = pd.DataFrame(data) # Create dataframe of numpy matrix\n",
    "data_df.columns = ['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','Y']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ApHfw7hWcpD"
   },
   "source": [
    "Now we split the data set into training, validation and testing samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5M9AlS7WNWF"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "x_train = pd.DataFrame(data[:n,0:d]).to_numpy()\n",
    "y_train = pd.DataFrame(data[:n,d]).to_numpy()\n",
    "\n",
    "# Validation data\n",
    "\n",
    "x_val = pd.DataFrame(data[n:n + m,0:d]).to_numpy()\n",
    "y_val = pd.DataFrame(data[n:n + m,d]).to_numpy()\n",
    "\n",
    "# Testing data\n",
    "\n",
    "x_test = pd.DataFrame(data[n + m:n + 2*m,0:d]).to_numpy()\n",
    "y_test = pd.DataFrame(data[n + m:n + 2*m,d]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyIsICmdWi1Y"
   },
   "source": [
    "# Neural Network Model\n",
    "\n",
    "We construct our neural network as a fully-connected, single-layer, feed-forward, regression neural network and compile it with the default Adam optimizer and the mean squared error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PPb8w4_PWGyq",
    "outputId": "d2e1528f-26aa-40f8-9c79-07eeab3008a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3125/3125 [==============================] - 2s 644us/step - loss: 7.6113 - val_loss: 0.3413\n",
      "Epoch 2/150\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 0.3051 - val_loss: 0.2597\n",
      "Epoch 3/150\n",
      "3125/3125 [==============================] - 2s 535us/step - loss: 0.2271 - val_loss: 0.1951\n",
      "Epoch 4/150\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 0.1878 - val_loss: 0.1573\n",
      "Epoch 5/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 0.1115 - val_loss: 0.0622\n",
      "Epoch 6/150\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 0.0489 - val_loss: 0.0391\n",
      "Epoch 7/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 0.0284 - val_loss: 0.0191\n",
      "Epoch 8/150\n",
      "3125/3125 [==============================] - 2s 509us/step - loss: 0.0174 - val_loss: 0.0139\n",
      "Epoch 9/150\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 0.0131 - val_loss: 0.0120\n",
      "Epoch 10/150\n",
      "3125/3125 [==============================] - 2s 517us/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 11/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 0.0086 - val_loss: 0.0078\n",
      "Epoch 12/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 0.0071 - val_loss: 0.0062\n",
      "Epoch 13/150\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 14/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 15/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 16/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 17/150\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 18/150\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 19/150\n",
      "3125/3125 [==============================] - 2s 517us/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 20/150\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 21/150\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 22/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 23/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 24/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 25/150\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 26/150\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 27/150\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 28/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 29/150\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 9.3947e-04 - val_loss: 9.1564e-04\n",
      "Epoch 30/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 7.2686e-04 - val_loss: 6.7444e-04\n",
      "Epoch 31/150\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 5.7021e-04 - val_loss: 5.0666e-04\n",
      "Epoch 32/150\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 4.6370e-04 - val_loss: 4.3150e-04\n",
      "Epoch 33/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 3.9157e-04 - val_loss: 3.6602e-04\n",
      "Epoch 34/150\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 3.4964e-04 - val_loss: 3.4994e-04\n",
      "Epoch 35/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 3.1691e-04 - val_loss: 3.4436e-04\n",
      "Epoch 36/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.9522e-04 - val_loss: 2.8899e-04\n",
      "Epoch 37/150\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 2.8119e-04 - val_loss: 2.5704e-04\n",
      "Epoch 38/150\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 2.7230e-04 - val_loss: 3.0262e-04\n",
      "Epoch 39/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 2.6533e-04 - val_loss: 2.5581e-04\n",
      "Epoch 40/150\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 2.5719e-04 - val_loss: 2.4842e-04\n",
      "Epoch 41/150\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 2.5411e-04 - val_loss: 2.3684e-04\n",
      "Epoch 42/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 2.5121e-04 - val_loss: 2.4794e-04\n",
      "Epoch 43/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.5202e-04 - val_loss: 2.5030e-04\n",
      "Epoch 44/150\n",
      "3125/3125 [==============================] - 2s 517us/step - loss: 2.4510e-04 - val_loss: 2.3244e-04\n",
      "Epoch 45/150\n",
      "3125/3125 [==============================] - 2s 510us/step - loss: 2.4224e-04 - val_loss: 2.2300e-04\n",
      "Epoch 46/150\n",
      "3125/3125 [==============================] - 2s 517us/step - loss: 2.4117e-04 - val_loss: 2.2052e-04\n",
      "Epoch 47/150\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 2.4137e-04 - val_loss: 2.3708e-04\n",
      "Epoch 48/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.3826e-04 - val_loss: 2.1867e-04\n",
      "Epoch 49/150\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 2.4002e-04 - val_loss: 2.8239e-04\n",
      "Epoch 50/150\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 2.3672e-04 - val_loss: 2.1212e-04\n",
      "Epoch 51/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.3465e-04 - val_loss: 2.3535e-04\n",
      "Epoch 52/150\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 2.3404e-04 - val_loss: 2.1666e-04\n",
      "Epoch 53/150\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 2.3450e-04 - val_loss: 2.6269e-04\n",
      "Epoch 54/150\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 2.3320e-04 - val_loss: 2.1299e-04\n",
      "Epoch 55/150\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 2.3352e-04 - val_loss: 2.1917e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f964689ec10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1) # Set seed for reproducibility\n",
    "\n",
    "# Neural network model\n",
    "\n",
    "neurons = 25 # 25 neurons in the hidden layer\n",
    "\n",
    "NN_model = tf.keras.models.Sequential()\n",
    "NN_model.add(tf.keras.layers.Dense(neurons, activation=tf.nn.sigmoid, input_dim = d)) \n",
    "NN_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "NN_model.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "# Training process\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=10**(-5), patience=5) # Early stopping criterion\n",
    "\n",
    "NN_model.fit(x_train, y_train, batch_size=32, epochs=150, validation_data=(x_val, y_val), callbacks=[callback], verbose=0) # Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQuUYGtgW3Ik"
   },
   "source": [
    "Now we evaluate our model by computing the loss on the validation and testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mjean7btW9aH",
    "outputId": "cda02231-1abb-42e4-e717-416cfa6a2756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 734us/step - loss: 2.1917e-04\n",
      "Loss on validation set is 0.00021917418052908033\n",
      "32/32 [==============================] - 0s 542us/step - loss: 2.2498e-04\n",
      "Loss on test set is 0.00022498385806102306\n"
     ]
    }
   ],
   "source": [
    "val_loss = NN_model.evaluate(x_val, y_val)\n",
    "print(\"Loss on validation set is\",val_loss)\n",
    "\n",
    "test_loss = NN_model.evaluate(x_test, y_test)\n",
    "print(\"Loss on test set is\",test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBU-dIkrFUhw"
   },
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "We compare the approximation quality of our neural network to an alternative linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "OzXpIVRJFXhQ",
    "outputId": "dbd1feec-5412-4467-cd73-ed084de6cf73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model parameters: \n",
      "            Coefficient\n",
      "X_1          -0.005766\n",
      "X_2          -0.000137\n",
      "X_3           0.005428\n",
      "X_4          -0.004201\n",
      "X_5           0.000292\n",
      "X_6          -0.006256\n",
      "X_7           0.101468\n",
      "X_8          -0.006968\n",
      "intercept    10.230597\n"
     ]
    }
   ],
   "source": [
    "LR_model = LinearRegression() # Linear regression model provided from scikit-learn\n",
    "LR_model.fit(x_train,y_train) # Fit training data set\n",
    "\n",
    "# Print model parameters\n",
    "\n",
    "param = np.append(LR_model.coef_,LR_model.intercept_)\n",
    "param_df = pd.DataFrame(param.transpose(), ['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','intercept'], columns=['Coefficient'])\n",
    "print('Linear regression model parameters: \\n',param_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n74xvDU7gp-b"
   },
   "source": [
    "Fitting the model and computing the MSE on the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IB6OVZnggfbk",
    "outputId": "9b2afbb5-4034-4674-ea16-cad57e1605e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.3383625773788169\n"
     ]
    }
   ],
   "source": [
    "y_pred = LR_model.predict(x_test) # Compute predictions for test set\n",
    "print('Test MSE:', metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8vpBVCQWTC1"
   },
   "source": [
    "# Empirical Neural Network Test Statistic\n",
    "\n",
    "Now we compute the empirical neural network test statistic. For that we first calculate the partial derivatives with respect to x_j over the training data set. First we compute the partial derivatives of the neural network with respect to each input variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HU1iZjX6XEJV"
   },
   "outputs": [],
   "source": [
    "x = tf.constant(x_train, dtype=tf.float32) # We regard the gradient over the training data set\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape: # Calculate gradient of the neural network\n",
    "    tape.watch(x)\n",
    "    y = NN_model(x)\n",
    "\n",
    "gradient = tape.gradient(y, x).numpy()\n",
    "\n",
    "del tape # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we compute the corresponding empirical expectations which represents the empirical neural network test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0kqV3DuQK9bB",
    "outputId": "88037402-599d-4e4c-99c1-19efa01df7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical neural network test statistic with respect to each variable: \n",
      " [1.290392 0.331923 0.33015  0.266212 0.492402 0.491897 0.010313 0.000017]\n"
     ]
    }
   ],
   "source": [
    "NN_test_stat = 1/n * (gradient**2).sum(axis=0) # Estimator of functional of neural network\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True) # Set precision of output\n",
    "\n",
    "print('Empirical neural network test statistic with respect to each variable: \\n',NN_test_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2XmEmcdjlPrZ"
   },
   "source": [
    "# Leave-One-Out Metric\n",
    "\n",
    "We compare the performance of our test statistic to the leave-one-out metric, which also evaluates variable influence. This metric is constructed by calculating the difference between the loss of the original model and the loss of the same model but fitted without the variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1onP0xASmOMQ",
    "outputId": "ed412475-da2e-424b-98d5-daca507f9f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable-index j = 0\n",
      "32/32 [==============================] - 0s 512us/step - loss: 0.0928\n",
      "Variable-index j = 1\n",
      "32/32 [==============================] - 0s 636us/step - loss: 0.1228\n",
      "Variable-index j = 2\n",
      "32/32 [==============================] - 0s 546us/step - loss: 0.1257\n",
      "Variable-index j = 3\n",
      "32/32 [==============================] - 0s 596us/step - loss: 0.0201\n",
      "Variable-index j = 4\n",
      "32/32 [==============================] - 0s 657us/step - loss: 0.1456\n",
      "Variable-index j = 5\n",
      "32/32 [==============================] - 0s 962us/step - loss: 0.1488\n",
      "Variable-index j = 6\n",
      "32/32 [==============================] - 0s 686us/step - loss: 0.0037\n",
      "Variable-index j = 7\n",
      "32/32 [==============================] - 0s 668us/step - loss: 2.4394e-04\n"
     ]
    }
   ],
   "source": [
    "leave_one_out_metric = np.zeros(8) # Initialize leave one out metric list\n",
    "\n",
    "for j in range(8): # Fit 8 neural networks, each without one different input variable\n",
    "    print('Variable-index j =',j)\n",
    "    # Heating issues of my hardware, added cool-down phase\n",
    "    # Drop values of the j.th variable X_j\n",
    "\n",
    "    x_train_j = np.delete(x_train,j,1)\n",
    "    x_val_j = np.delete(x_val,j,1)\n",
    "    x_test_j = np.delete(x_test,j,1)\n",
    "\n",
    "    # Neural network model\n",
    "\n",
    "    NN_model_j = tf.keras.models.Sequential()\n",
    "    NN_model_j.add(tf.keras.layers.Dense(25, activation=tf.nn.sigmoid, input_dim = d - 1))\n",
    "    NN_model_j.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    NN_model_j.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "    # Training process\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=10**(-5), patience=5) # Early stopping criterion\n",
    "\n",
    "    NN_model_j.fit(x_train_j, y_train, batch_size=32, epochs=150, validation_data=(x_val_j, y_val), callbacks=[callback], verbose=0)\n",
    "\n",
    "    # Compute test MSE and difference to standard NN_model\n",
    "\n",
    "    test_loss_j = NN_model_j.evaluate(x_test_j, y_test)\n",
    "    leave_one_out_metric[j] = abs(test_loss - test_loss_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hdlxb4pFv30T",
    "outputId": "7644853f-6152-4520-afb9-80bbba10214d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave one out metric with respect to each variable: \n",
      " [0.092588 0.12255  0.125523 0.019852 0.145383 0.148606 0.00344  0.000019]\n"
     ]
    }
   ],
   "source": [
    "print('Leave one out metric with respect to each variable: \\n',leave_one_out_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzmUhx0q3v6w"
   },
   "source": [
    "# Quantile Estimation\n",
    "\n",
    "We now estimates the quantile of the limiting distribution of the neural network test statistic. The first approach is using the series representation and the second approach uses the discretization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series Representation Approach\n",
    "\n",
    "We frist compute the truncated sums in the nominator and denominator. The truncation order N indicates that the sum over all elements z = (z_1,...,z_d) in Z^d is truncated in a way such that we omitted z with |z_j| > N for some 1 ≤ j ≤ d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting computation constants for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1bofGJtZ3zt2",
    "outputId": "7d5818ed-8eff-4534-9279-e0889162f40a"
   },
   "outputs": [],
   "source": [
    "N = 1 # Truncation order\n",
    "m_N = 10000 # Number of samples\n",
    "s = d//2 + 2 # Sobolev norm index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genrate samples of the chi-squared distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1bofGJtZ3zt2",
    "outputId": "7d5818ed-8eff-4534-9279-e0889162f40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling process took 12.811657905578613 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # Measuring the computing time of the chi-squared sampling process\n",
    "\n",
    "x_iz = np.random.chisquare(1,(m_N,2**d,(2*N + 1)**d))\n",
    "\n",
    "end = time.time()\n",
    "print('Sampling process took',end - start,'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and label elements in the sets {0,1}^d and Z^d such that we can iterate through them using the natural numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1bofGJtZ3zt2",
    "outputId": "7d5818ed-8eff-4534-9279-e0889162f40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating indexes. Shapes: (6561, 8) (256, 8)\n"
     ]
    }
   ],
   "source": [
    "z_nums = np.zeros((((2*N + 1)**d),d)) # Placeholder for elements in Z^d\n",
    "itr = 0 # Iteration counter\n",
    "for z in itertools.product(range(-N,N + 1), repeat=d): # This line generates the actual tuples\n",
    "    z_nums[itr,:] = np.array(z) # Placing the generated tuple\n",
    "    itr = itr + 1\n",
    "\n",
    "i_nums = np.zeros(((2**d),d)) # Placeholder for elements in {0,1}^d\n",
    "itr = 0 # Resetting iteration counter\n",
    "for i in itertools.product(range(2), repeat=d): # This line generates the actual tuples\n",
    "    i_nums[itr,:] = np.array(i) # Placing the generated tuple\n",
    "    itr = itr + 1\n",
    "\n",
    "print('Finished generating indexes. Shapes:',z_nums.shape,i_nums.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the weigths d_{i,z}^2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the weight array: (256, 6561)\n"
     ]
    }
   ],
   "source": [
    "# First compute weights d_{i,z}^2 for a fixed i \\in {0,1}^d\n",
    "\n",
    "d_z = np.zeros((2*N + 1)**d) # Placeholder for weights d_{i,z}^2 for fixed i\n",
    "\n",
    "for a in itertools.product(range(s), repeat=d): # Generate multiindex alpha\n",
    "    if sum(list(a)) <= s: # Only consider |alpha| <= s\n",
    "        d_z = d_z + np.prod((np.pi*z_nums)**(np.multiply(2,a)), axis=1) # Increase sum\n",
    "\n",
    "# Compute d_{i,z}^2\n",
    "\n",
    "d_iz = np.matlib.repmat(d_z,256,1) # The weights are identical for different i so we replicate them\n",
    "\n",
    "print('Shape of the weight array:',d_iz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the truncated sum in the denominator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different samples of the truncated sum in the denominator: \n",
      " (100,)\n"
     ]
    }
   ],
   "source": [
    "denom_sum = np.sum(x_iz/d_iz, axis=(1,2))\n",
    "print('Different samples of the truncated sum in the denominator: \\n',denom_sum.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with calculating the weights alpha_{i,z,j}^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6561, 8)\n",
      "(256, 6561, 8)\n"
     ]
    }
   ],
   "source": [
    "alpha_zj = (np.pi*z_nums)**2 # alpha_{z,j}^2 for fixed i and every j\n",
    "\n",
    "print(alpha_zj.shape)\n",
    "\n",
    "alpha_izj = np.tile(alpha_zj,(2**d,1,1)) # Identical for different i, so we copy it 2^d times\n",
    "\n",
    "print(alpha_izj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the truncated sum in the nominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing nominator sum took 33.301169872283936 seconds.\n",
      "Different samples of the truncated sum in the nominator with respect to each variable: \n",
      " (100, 8)\n"
     ]
    }
   ],
   "source": [
    "frac = x_iz/(d_iz**2)\n",
    "prod = np.einsum('abc,xab->xabc', alpha_izj,frac)\n",
    "\n",
    "start = time.time() # Measuring the computing time of the nominator sum\n",
    "\n",
    "nom_sum = np.sum(prod, axis=(1,2))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Computing nominator sum took',end - start,'seconds.')\n",
    "print('Different samples of the truncated sum in the nominator with respect to each variable: \\n',nom_sum.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we can already calculate samples from the limiting distribution and scaling factor B = 1. Then we just have to scale the samples with B^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscaled samples of the limiting distribution with respect to each variable: (very small) \n",
      " (100, 8)\n"
     ]
    }
   ],
   "source": [
    "unscaled_Z = nom_sum/denom_sum[:,np.newaxis] # Samples from the limiting distribution with B = 1\n",
    "print('Unscaled samples of the limiting distribution with respect to each variable: (very small) \\n',unscaled_Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating B\n",
    "\n",
    "To obtain samples from the series representation it remains to choose the scaling factor B^2. Therefore we need a constant B that dominates the Sobolev norm of the unknown regression function f_0 while being as small as possible. Thus one could set B equal to the Sobolev norm for f_0. Typically f_0 is unknown but we start with explicitly calculating its Sobolev norm partly by hand and partly by numerical integration. We will use this as a reference for our B estimator.\n",
    "\n",
    "To compute the Sobolev norm, we have to compute the squared L^2 norm for every partial derivative explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = np.zeros(28) # Placeholder for the squared L^2 norms of the partial derivatives, only 28 are not zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the squared L^2 norm of f_0. We estimate it using the empirial expectation of f_n on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation for the squared L^2 norm of f_0: 10501646.59098053\n"
     ]
    }
   ],
   "source": [
    "f_n = NN_model.predict(x_train)[:,0]\n",
    "ev[0] = sum(f_n**2)\n",
    "print('Estimation for the squared L^2 norm of f_0:',ev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next squared partial derivative expectations were calculated by hand. By the symmetry of certain variables (e.g. X_5 and X_6) we double or tripple certain expectations that occur two or three times in the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev[1] = 4/3\n",
    "ev[2] = 4\n",
    "ev[3] = 2 * 1/3 # 2 times\n",
    "ev[4] = 1\n",
    "ev[5] = 3 * (0.5 - 0.25*np.sin(2)) # 3 times\n",
    "ev[6] = 3 * (np.cos(1)*np.sin(1)/2 + 0.5) # 3 times\n",
    "ev[7] = 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the missing partial derivatives using scipy's integration tools. If some expectations occur more than once in the sum, we scale the integrand by the number of occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "from scipy.integrate import dblquad\n",
    "from scipy.integrate import tplquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = x_5, x = x_6\n",
    "# Probability density function: 1/(2^d), so only 1/4 for the two variables X_5 and X_6\n",
    "# Define the integrands:\n",
    "\n",
    "def int1(y, x):\n",
    "    return ((y*np.exp(x*y))**2)/4\n",
    "\n",
    "def int2(y, x):\n",
    "    return (((y**2)*np.exp(x*y))**2)/4\n",
    "\n",
    "def int3(y, x):\n",
    "    return (((y**3)*np.exp(x*y))**2)/4\n",
    "\n",
    "def int4(y, x):\n",
    "    return (((y**4)*np.exp(x*y))**2)/4\n",
    "\n",
    "def int5(y, x):\n",
    "    return (((y**5)*np.exp(x*y))**2)/4\n",
    "\n",
    "def int6(y, x):\n",
    "    return (((y**6)*np.exp(x*y))**2)/4\n",
    "\n",
    "def int7(y, x):\n",
    "    return (((1 + x*y)*np.exp(x*y))**2)/4\n",
    "\n",
    "def int8(y, x): # 2 times\n",
    "    return (((2*y + (y**2)*x)*np.exp(y*x))**2)/2\n",
    "\n",
    "def int9(y, x): # 2 times\n",
    "    return (((2*(y**2) + x*(y**3))*np.exp(y*x))**2)/2\n",
    "\n",
    "def int10(y, x): # 2 times\n",
    "    return (((2*(y**3) + x*(y**4))*np.exp(y*x))**2)/2\n",
    "\n",
    "def int11(y, x): # 2 times\n",
    "    return (((2*(y**4) + x*(y**5))*np.exp(y*x))**2)/2\n",
    "\n",
    "def int12(y, x): \n",
    "    return (((2 + 4*y*x + (y*x)**2)*np.exp(y*x))**2)/4\n",
    "\n",
    "def int13(y, x): # 2 times\n",
    "    return (((6*y + 6*(y**2)*x + (y*x)**2)*np.exp(y*x))**2)/2\n",
    "\n",
    "def int14(y, x): # 2 times\n",
    "    return (((12*(y**2) + 8*(y**3)*x + (y**4)*(x**2))*np.exp(y*x))**2)/2\n",
    "\n",
    "def int15(y, x):\n",
    "    return (((6 + 18*y*x + 9*((y*x)**2) + (y*x)**3)*np.exp(y*x))**2)/4\n",
    "\n",
    "# List of the integrands for iteration:\n",
    "int_list = [int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, int14, int15]\n",
    "\n",
    "# Computing the integrals:\n",
    "for i in range(15):\n",
    "    ev[13 + i] = dblquad(int_list[i], -1, 1, lambda x: -1, lambda x: 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to sum all expectations together and take the square root to obtain an estimator for the Sobolev norm of f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation for the sobolev norm of f_0: 3240.7039410538114\n"
     ]
    }
   ],
   "source": [
    "sob_norm = np.sqrt(sum(ev)) # Desired B value, Sobolev norm of f_0\n",
    "print('Estimation for the sobolev norm of f_0:',sob_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nt = 100\\nzx_comb = np.einsum('ab,xb->axb', z_nums, x_train[:t,:]) # Every possible combination of z and x_train values\\n# Compute cosine and sine values of each combination\\ncos_val = np.cos(np.pi*zx_comb)\\nsin_val = np.sin(np.pi*zx_comb)\\n\\nnot_i_nums = 1 - i_nums\\n\\nizx_cos_comb = np.einsum('ab,xyb->axyb', not_i_nums, cos_val)\\nizx_sin_comb = np.einsum('ab,xyb->axyb', i_nums, cos_val)\\n\\nphi_array = izx_cos_comb + izx_sin_comb\\n\\nphi = np.prod(phi_array, axis=3)\\n\\nphi.shape\\n\\nmodel_val = NN_model.predict(x_train[:t,:])[:,0]\\nmodel_val.shape\\n\\nskalarprod = np.sum(phi*model_val, axis=2)/t\\n\\nB = np.sqrt(np.sum(d_iz*(skalarprod**2)))\\nprint(B\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 100\n",
    "zx_comb = np.einsum('ab,xb->axb', z_nums, x_train[:t,:]) # Every possible combination of z and x_train values\n",
    "# Compute cosine and sine values of each combination\n",
    "cos_val = np.cos(np.pi*zx_comb)\n",
    "sin_val = np.sin(np.pi*zx_comb)\n",
    "\n",
    "not_i_nums = 1 - i_nums\n",
    "\n",
    "izx_cos_comb = np.einsum('ab,xyb->axyb', not_i_nums, cos_val)\n",
    "izx_sin_comb = np.einsum('ab,xyb->axyb', i_nums, cos_val)\n",
    "\n",
    "phi_array = izx_cos_comb + izx_sin_comb\n",
    "\n",
    "phi = np.prod(phi_array, axis=3)\n",
    "\n",
    "phi.shape\n",
    "\n",
    "model_val = NN_model.predict(x_train[:t,:])[:,0]\n",
    "model_val.shape\n",
    "\n",
    "skalarprod = np.sum(phi*model_val, axis=2)/t\n",
    "\n",
    "B = np.sqrt(np.sum(d_iz*(skalarprod**2)))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating B by Sampling Neural Networks\n",
    "\n",
    "This method is computationally inefficient as it requires fitting several neural networks to different data sets. Later we require fitting multiple networks again for estimating the test's performance and since these tasks coincide, we refer to the section \"Performance of the test\" for the computation of the neural network test statistic samples. Using these samples we can calculate the empirical expectation and determine the B estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples of the limiting distribution of the neural network test statistic: \n",
      " (100, 8)\n"
     ]
    }
   ],
   "source": [
    "B = 643.3893036066657 # Obtained from later implementation\n",
    "\n",
    "# Multiply unscaled samples from the limiting distribution by B^2\n",
    "\n",
    "Z = (B**2)*unscaled_Z\n",
    "print('Samples of the limiting distribution of the neural network test statistic: \\n',Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order statistic index i = 95\n",
      "Quantiles Z_i with respect to each variable: \n",
      " [0.000898 0.000889 0.000881 0.000902 0.000893 0.000932 0.000909 0.000898]\n"
     ]
    }
   ],
   "source": [
    "order_Z = np.sort(Z, axis=0) # Order statistics of samples\n",
    "\n",
    "alpha = 0.05 # Confidence Level\n",
    "\n",
    "# Find empirical quantile\n",
    "\n",
    "intervals = np.linspace(0,1,m_N + 1) # Array m_N probability values from 0 increasing to 1\n",
    "i = np.searchsorted(intervals, 1 - alpha) # Find index where 1 - alpha would be sorted in to maintain increasing order\n",
    "print('Order statistic index i =',i)\n",
    "SNNquantiles = order_Z[i - 1][:] # Quantiles by using sampled neural networks to calculate B\n",
    "print('Quantiles Z_i with respect to each variable: \\n',SNNquantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results (Series Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: [1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_bool_array = ((neurons**2)*NN_test_stat > SNNquantiles).astype(int) # Check if the scaled neural network test statistic  exceeds the quantile\n",
    "print('Test result:',test_bool_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization Method\n",
    "\n",
    "This method is computationally far superior as it does not require re-fitting the neural network. To obtain samples from the limiting distribution, we need to identify the argmax of the Gaussian process indexed by the function space \\Theta. We achieve this by approximation the infinite dimensional function space with an epsilon cover. This cover only requires a finite number of functions from \\Theta. We randomly generate functions from \\Theta and by generating a large enough number, the probabilty of covering \\Theta increases. For our implementation we use 500 randomly generated functions, which in our case are neural networks with randomly generated parameters. Now comes the sampling process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrf = 500 # Number of random functions we sample\n",
    "rnd_fcts = [] # Placeholder for random functions\n",
    "\n",
    "for i in range(nrf): # Generate neural networks with randomly sampled parameters\n",
    "    rnd_fcts.append(tf.keras.models.Sequential())\n",
    "    rnd_fcts[i].add(tf.keras.layers.Dense(25, activation=tf.nn.sigmoid, input_dim = d, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')) # Glorot normal distribution for model parameters\n",
    "    rnd_fcts[i].add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to sample from the limiting Gaussian process indexed by our 500 functions. The mean is determined by zero so it remains to compute the covariance matrix. We start by evaluating the 500 functions h on the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_evals = np.zeros((nrf,n)) # Placeholder for evaluation of random functions h\n",
    "\n",
    "for i in range(nrf): # Evaluate random functions\n",
    "    h_evals[i] = rnd_fcts[i].predict(x_train)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we can determine the covariance estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sum = h_evals @ h_evals.T # Compute sum of h_i(X_k)h_j(X_k) for fixed i,j\n",
    "cov = 4*(sigma**2)*h_sum/n # Calculate covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample 1000 times from the 500 dimensional multivariate Gaussian distribution and identify the argmax for each of the 1000 sample vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros(nrf) # Mean vector of the Gaussian process\n",
    "nas = 10000 # Number of argmax samples\n",
    "\n",
    "g_process = np.random.multivariate_normal(zeros, cov, nas) # Sample from limiting Gaussian process\n",
    "arg_idx = np.argmax(g_process, axis=1) # Find argmax index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compute the expected value of the squared partial derivatives of the argmax function. We first evaluate the gradient on the training data set for each of our 500 functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = np.zeros((nrf,n,d)) # Placeholder for gradients of all 500 functions\n",
    "x = tf.constant(x_train, dtype=tf.float32) # We regard the gradient over the training data set\n",
    "\n",
    "for i in range(nrf): # Calculate gradients of all 500 functions\n",
    "    with tf.GradientTape(persistent=True) as tape: # Calculate gradient of the neural network\n",
    "        tape.watch(x)\n",
    "        y = rnd_fcts[i](x)\n",
    "\n",
    "    gradients[i] = tape.gradient(y, x).numpy()\n",
    "\n",
    "del tape  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only consider the evaluated gradients for functions that actually appear as an argmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "argm_gradients = np.zeros((nas,n,d)) # Placeholder for gradients of argmax functions only\n",
    "\n",
    "for i in range(nas): # Insert gradients of argmax functions\n",
    "    argm_gradients[i] = gradients[arg_idx[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the estimator for the expection of the squared partial derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n"
     ]
    }
   ],
   "source": [
    "Z = 1/n * (argm_gradients**2).sum(axis=1)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 1000 samples of the limiting distribution for each coordinate 1 ≤ j ≤ d. Therefore we now determine the empirical quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order statistic index i = 95\n",
      "Quantiles Z_i with respect to each variable: \n",
      " [0.000898 0.000889 0.000881 0.000902 0.000893 0.000932 0.000909 0.000898]\n"
     ]
    }
   ],
   "source": [
    "intervals = np.linspace(0,1,m_N + 1) # Array m_N probability values from 0 increasing to 1\n",
    "i = np.searchsorted(intervals, 1 - alpha) # Find index where 1 - alpha would be sorted in to maintain increasing order\n",
    "print('Order statistic index i =',i)\n",
    "quantiles = order_Z[i - 1][:]\n",
    "print('Quantiles Z_i with respect to each variable: \\n',quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results (Discretization Method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: [1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_bool_array = (25*NN_test_stat > quantiles).astype(int)\n",
    "print('Test result:',test_bool_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the Test\n",
    "\n",
    "To evaluate the performance of the test, we estimate its power and size by performing it on 100 alternative training data sets. Recall:\n",
    "\n",
    "**Power**: Probability that the null hypothesis is rejected when the alternative is true.\\\n",
    "**Size**: Probabilty that the null hypothesis is rejected when the null is true.\n",
    "\n",
    "Therefore the expected value of the test statistic for X_8 represents its power and the expected value of the test for all other feature variables represents its size. We first generate the new 100 alternative data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.044155</td>\n",
       "      <td>0.089165</td>\n",
       "      <td>-0.555221</td>\n",
       "      <td>0.712626</td>\n",
       "      <td>-0.252611</td>\n",
       "      <td>0.505551</td>\n",
       "      <td>-0.704385</td>\n",
       "      <td>0.075142</td>\n",
       "      <td>9.511502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.076891</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>-0.900746</td>\n",
       "      <td>0.826395</td>\n",
       "      <td>0.213613</td>\n",
       "      <td>0.146388</td>\n",
       "      <td>0.324458</td>\n",
       "      <td>-0.106687</td>\n",
       "      <td>9.283602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.720737</td>\n",
       "      <td>0.925285</td>\n",
       "      <td>0.910770</td>\n",
       "      <td>0.057433</td>\n",
       "      <td>-0.024372</td>\n",
       "      <td>0.580635</td>\n",
       "      <td>-0.378254</td>\n",
       "      <td>-0.950956</td>\n",
       "      <td>11.312323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760672</td>\n",
       "      <td>0.057912</td>\n",
       "      <td>-0.480058</td>\n",
       "      <td>0.595180</td>\n",
       "      <td>0.667535</td>\n",
       "      <td>0.747551</td>\n",
       "      <td>-0.075823</td>\n",
       "      <td>-0.474159</td>\n",
       "      <td>11.025729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.204379</td>\n",
       "      <td>0.367499</td>\n",
       "      <td>0.264937</td>\n",
       "      <td>0.873847</td>\n",
       "      <td>-0.493808</td>\n",
       "      <td>-0.055887</td>\n",
       "      <td>0.891440</td>\n",
       "      <td>9.840353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>-0.083593</td>\n",
       "      <td>-0.950088</td>\n",
       "      <td>-0.336528</td>\n",
       "      <td>-0.872823</td>\n",
       "      <td>-0.772132</td>\n",
       "      <td>-0.867780</td>\n",
       "      <td>-0.610100</td>\n",
       "      <td>-0.327872</td>\n",
       "      <td>10.861105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>-0.677511</td>\n",
       "      <td>0.059008</td>\n",
       "      <td>-0.699660</td>\n",
       "      <td>0.012592</td>\n",
       "      <td>-0.356464</td>\n",
       "      <td>0.871729</td>\n",
       "      <td>-0.916914</td>\n",
       "      <td>0.751804</td>\n",
       "      <td>10.061728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0.266204</td>\n",
       "      <td>-0.657625</td>\n",
       "      <td>-0.860812</td>\n",
       "      <td>0.114747</td>\n",
       "      <td>0.218639</td>\n",
       "      <td>-0.260168</td>\n",
       "      <td>-0.788328</td>\n",
       "      <td>0.656879</td>\n",
       "      <td>10.457416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>-0.362661</td>\n",
       "      <td>-0.309295</td>\n",
       "      <td>0.846551</td>\n",
       "      <td>0.101806</td>\n",
       "      <td>-0.449618</td>\n",
       "      <td>0.621645</td>\n",
       "      <td>0.452097</td>\n",
       "      <td>0.549557</td>\n",
       "      <td>9.674069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>-0.834668</td>\n",
       "      <td>-0.406382</td>\n",
       "      <td>0.461035</td>\n",
       "      <td>-0.698708</td>\n",
       "      <td>0.088467</td>\n",
       "      <td>-0.115673</td>\n",
       "      <td>0.758851</td>\n",
       "      <td>-0.894068</td>\n",
       "      <td>10.340346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X_1       X_2       X_3       X_4       X_5       X_6       X_7  \\\n",
       "0      -0.044155  0.089165 -0.555221  0.712626 -0.252611  0.505551 -0.704385   \n",
       "1      -0.076891  0.511528 -0.900746  0.826395  0.213613  0.146388  0.324458   \n",
       "2       0.720737  0.925285  0.910770  0.057433 -0.024372  0.580635 -0.378254   \n",
       "3       0.760672  0.057912 -0.480058  0.595180  0.667535  0.747551 -0.075823   \n",
       "4       0.403479  0.204379  0.367499  0.264937  0.873847 -0.493808 -0.055887   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "999995 -0.083593 -0.950088 -0.336528 -0.872823 -0.772132 -0.867780 -0.610100   \n",
       "999996 -0.677511  0.059008 -0.699660  0.012592 -0.356464  0.871729 -0.916914   \n",
       "999997  0.266204 -0.657625 -0.860812  0.114747  0.218639 -0.260168 -0.788328   \n",
       "999998 -0.362661 -0.309295  0.846551  0.101806 -0.449618  0.621645  0.452097   \n",
       "999999 -0.834668 -0.406382  0.461035 -0.698708  0.088467 -0.115673  0.758851   \n",
       "\n",
       "             X_8          Y  \n",
       "0       0.075142   9.511502  \n",
       "1      -0.106687   9.283602  \n",
       "2      -0.950956  11.312323  \n",
       "3      -0.474159  11.025729  \n",
       "4       0.891440   9.840353  \n",
       "...          ...        ...  \n",
       "999995 -0.327872  10.861105  \n",
       "999996  0.751804  10.061728  \n",
       "999997  0.656879  10.457416  \n",
       "999998  0.549557   9.674069  \n",
       "999999 -0.894068  10.340346  \n",
       "\n",
       "[1000000 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 250 # Number of data sets\n",
    "# Other constants are defined in the first data generation section\n",
    "\n",
    "X = np.random.uniform(-1,1,(num*n,d)) # 100*n samples of 8 dimensional random vector X uniformly distributed on [-1,1]\n",
    "eps = np.random.normal(0,sigma,num*n) # Gaussian noise\n",
    "\n",
    "Y = 8 + X[:,0]**2 + X[:,1]*X[:,2] + np.cos(X[:,3]) + np.exp(X[:,4]*X[:,5]) + 0.1*X[:,6] + eps # Output computation\n",
    "Y = Y.reshape((-1,1)) # Transpose output vector\n",
    "\n",
    "data = np.concatenate((X,Y), axis=1) # First 8 columns as input and last column as output\n",
    "data_df = pd.DataFrame(data) # Create dataframe of numpy matrix\n",
    "data_df.columns = ['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','Y']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split generated data into 100 training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100000, 8)\n",
      "(10, 100000, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(np.split(pd.DataFrame(data[:,0:8]).to_numpy(), num)) # x_train as array of 100 training data sets\n",
    "y_train = np.array(np.split(pd.DataFrame(data[:,8]).to_numpy(), num)) # y_train as array of 100 training data sets\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit 100 neural networks to its corresponding training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n"
     ]
    }
   ],
   "source": [
    "NN_models = []\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=10**(-5), patience=5) # Early stopping criterion\n",
    "\n",
    "for i in range(num):\n",
    "    print('Iteration:',i + 1)\n",
    "    NN_models.append(tf.keras.models.Sequential())\n",
    "    NN_models[i].add(tf.keras.layers.Dense(neurons, activation=tf.nn.sigmoid, input_dim = d)) \n",
    "    NN_models[i].add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    NN_models[i].compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "    # Training process\n",
    "\n",
    "    NN_models[i].fit(x_train[i], y_train[i], batch_size=32, epochs=150, validation_data=(x_val, y_val), callbacks=[callback], verbose=0) # Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients for the neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = np.zeros((num,n,d))\n",
    "\n",
    "for i in range(num):\n",
    "    x = tf.constant(x_train[i], dtype=tf.float32) # We regard the gradient over the corresponding training data set\n",
    "    with tf.GradientTape(persistent=True) as tape: # Calculate gradient of the neural network\n",
    "        tape.watch(x)\n",
    "        y = NN_models[i](x)\n",
    "\n",
    "    gradients[i] = tape.gradient(y, x).numpy()\n",
    "\n",
    "    del tape # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the neural network test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_test_stats = 1/n * (gradients**2).sum(axis=1) # Estimator for functional of neural network\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True) # Set precision of output\n",
    "\n",
    "print('Empirical neural network test statistic with respect to each variable: \\n',NN_test_stats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bool_arrays = (neurons*NN_test_stats > quantiles).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to evaluate the power and size of the test we approximate the test's expectation by using the empirical expectation over the 250 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_size = test_bool_arrays.sum(axis=0)/num # Average the test results over the 100 samples to approximate the test's expected value\n",
    "print(power_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the B Estimator With Sampled Neural Network Test Statistics\n",
    "\n",
    "The normal procedure goes as follows: We add a noise variable X_{d+1} in a new column to our data set. The corresponding new samples should possess a similar distribution to our original data. The next step is to split our data into a few alternative data sets. We use them to fit multiple nerual networks to each data set and compute the test statistic for X_{d+1}. Using this method we obtain (a few) samples of the neural network test statistic. Further, even if we do not know the regression function f_0, it is clear the the new variable X_{d+1} satisfies the null hypothesis, i.e. it is irrelevant for the regression output. Thus the convergence results apply and we have approxiamte samples of the limiting distribution. These samples are scaled by B^2. Now we generate samples from the unscaled series representation. By taking the average of the NN-test statistic samples and dividing by the average of the unscaled series representation samples, we obtain B^2. Note that this estimator is very noisy because creating NN-test statistic samples requires splitting our limited supply of training data.\n",
    "\n",
    "Since repeating the prior process for an additional dimension is such a heavy task, we use X_8 as our noise variable. It possesses the same cahracteristics as any other noise variable mentioned above. Note however, that typically we do not know which input variables satisfy the null hypotheses. This is just a demonstration example.\n",
    "\n",
    "Another point to mention is that in order to increase the likelihood of covering the Sobolev norm of f_0 with B, we take the maximum value of the NN-test statistic samples instead of the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_NN_test_stats_max = (neurons**2)*np.max(NN_test_stats[:,7]) # Only X_8 suffices the null hypothesis\n",
    "\n",
    "unscaled_Z_mean = np.sum(unscaled_Z[:,7])\n",
    "\n",
    "B = np.sqrt(scaled_NN_test_stats_max/unscaled_Z_mean)\n",
    "\n",
    "print('B estimator:',B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-test for Linear Regression Model\n",
    "\n",
    "Now we evaluate the standard t-test for a linear regression model. For that we fit multiple models and thus obatin multiple samples of the t-test, which we need to estimate its power and size. This time we use the statsmodel library which provides a t-test for each variable with our null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "pvalues = np.zeros((num,d)) # Placeholder for probabilities of t-test\n",
    "\n",
    "for i in range(num): # Generate multiple samples of t-test\n",
    "    X = sm.add_constant(x_train[i]) # Statsmodels library requires one extra variable for intercept of linear regression\n",
    "    results = sm.OLS(y_train[i], X).fit() # Fitting the linear regression model to its corresponding data set\n",
    "    pvalues[i] = results.pvalues[1:] # Copy probabilities of t-test, pvalue of intercept not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check if the pvalues of the test statistic are below our significance level alpha = 0.05. This represents the result of the t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_test_bool_array = (pvalues <= 0.05).astype(int) # Check if probability is below our alpha\n",
    "power_size = lin_test_bool_array.sum(axis=0)/num # average the t-test samples to obtain empirical expectation\n",
    "print(power_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlated Feature Variable\n",
    "\n",
    "In this last simulation experiment we investigate the robustness of our NN-test if the feature variable possesses a dependece structure. More specifically, we use a Gaussian copula with the same uniform margins as before. We obtain this distribution by sampling vectors from a multivariate normal distribution and applying a normal distribution function to each vector coordinate. Then Sklar's theorem ensures the uniformity of the marginal distributions. Our objective is again to evaluate the test's performance by estimating its power and size. First we define the mean and covariance matrix of the multivariate Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm # TODO\n",
    "\n",
    "mean_zero = np.zeros(d) # Mean vector for multivariate normal distribution\n",
    "\n",
    "# Define covariance matrix for multivariate normal distribution\n",
    "\n",
    "Sig = np.eye(d)\n",
    "Sig[0,1],Sig[1,0] = 0.1, 0.1\n",
    "Sig[4,5],Sig[5,4] = 0.5, 0.5\n",
    "Sig[3,6],Sig[6,3] = 0.3, 0.3\n",
    "\n",
    "print(Sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate our data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000 # number of multivariate normal samples\n",
    "\n",
    "mn_samples = np.random.multivariate_normal(mean_zero, Sig, num*n + m) # n samples from multivariate normal distribution\n",
    "X = 2*norm.cdf(mn_samples) - 1 # Apply standard normal df to obtain uniform marginal distributions\n",
    "eps = np.random.normal(0,sigma,num*n + m) # Gaussian noise\n",
    "\n",
    "Y = 8 + X[:,0]**2 + X[:,1]*X[:,2] + np.cos(X[:,3]) + np.exp(X[:,4]*X[:,5]) + 0.1*X[:,6] + eps # Output computation\n",
    "Y = Y.reshape((-1,1)) # Transpose output vector\n",
    "\n",
    "data = np.concatenate((X,Y), axis=1) # First 8 columns as input and last column as output\n",
    "data_df = pd.DataFrame(data) # Create dataframe of numpy matrix\n",
    "data_df.columns = ['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','Y']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data set into multiple training sets and a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data sets\n",
    "\n",
    "x_train = np.array(np.split(pd.DataFrame(data[:n,0:d]).to_numpy(), num)) # x_train as array of 100 training data sets\n",
    "y_train = np.array(np.split(pd.DataFrame(data[:n,d]).to_numpy(), num)) # y_train as array of 100 training data sets\n",
    "\n",
    "# Validation data set\n",
    "\n",
    "x_val = pd.DataFrame(data[n:n + m,0:d]).to_numpy()\n",
    "y_val = pd.DataFrame(data[n:n + m,d]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_models = []\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=10**(-5), patience=5) # Early stopping criterion\n",
    "\n",
    "for i in range(num):\n",
    "    print('Iteration:',i + 1)\n",
    "    \n",
    "    # Model construction\n",
    "    \n",
    "    NN_models.append(tf.keras.models.Sequential())\n",
    "    NN_models[i].add(tf.keras.layers.Dense(neurons, activation=tf.nn.sigmoid, input_dim = d)) \n",
    "    NN_models[i].add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    NN_models[i].compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "    # Training process\n",
    "    NN_models[i].fit(x_train[i], y_train[i], batch_size=32, epochs=150, validation_data=(x_val, y_val), callbacks=[callback], verbose=0) # Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the gradients for each network over its training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = np.zeros((num,n,d))\n",
    "\n",
    "for i in range(num):\n",
    "    x = tf.constant(x_train[i], dtype=tf.float32) # We regard the gradient over the corresponding training data set\n",
    "    with tf.GradientTape(persistent=True) as tape: # Calculate gradient of the neural network\n",
    "        tape.watch(x)\n",
    "        y = NN_models[i](x)\n",
    "\n",
    "    gradients[i] = tape.gradient(y, x).numpy()\n",
    "\n",
    "    del tape # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the expected value of the squared partial derivatives for each network, which represents the test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_test_stats = 1/n * (gradients**2).sum(axis=1) # Estimator for functional of neural network\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True) # Set precision of output\n",
    "\n",
    "print('Empirical neural network test statistic with respect to each variable: \\n',NN_test_stats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check if the scaled NN-test statistic exceeds its quantile. This also represents the outcome of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bool_arrays = (neurons*NN_test_stats > quantiles).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly we average the test results to approximate the test's expected value, which represents its power and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_size = test_bool_arrays.sum(axis=0)/num # Average the test results over the 100 samples to approximate the test's expected value\n",
    "print(power_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SignificanceTests.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
