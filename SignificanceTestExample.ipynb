{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "import numpy.matlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.165956</td>\n",
       "      <td>0.440649</td>\n",
       "      <td>-0.999771</td>\n",
       "      <td>-0.395335</td>\n",
       "      <td>-0.706488</td>\n",
       "      <td>-0.815323</td>\n",
       "      <td>-0.627480</td>\n",
       "      <td>-0.308879</td>\n",
       "      <td>10.218395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.206465</td>\n",
       "      <td>0.077633</td>\n",
       "      <td>-0.161611</td>\n",
       "      <td>0.370439</td>\n",
       "      <td>-0.591096</td>\n",
       "      <td>0.756235</td>\n",
       "      <td>-0.945225</td>\n",
       "      <td>0.340935</td>\n",
       "      <td>9.505227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.165390</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>-0.719226</td>\n",
       "      <td>-0.603797</td>\n",
       "      <td>0.601489</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>-0.373152</td>\n",
       "      <td>0.384645</td>\n",
       "      <td>10.492027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.752778</td>\n",
       "      <td>0.789213</td>\n",
       "      <td>-0.829912</td>\n",
       "      <td>-0.921890</td>\n",
       "      <td>-0.660339</td>\n",
       "      <td>0.756285</td>\n",
       "      <td>-0.803306</td>\n",
       "      <td>-0.157785</td>\n",
       "      <td>9.026440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.915779</td>\n",
       "      <td>0.066331</td>\n",
       "      <td>0.383754</td>\n",
       "      <td>-0.368969</td>\n",
       "      <td>0.373002</td>\n",
       "      <td>0.669251</td>\n",
       "      <td>-0.963423</td>\n",
       "      <td>0.500289</td>\n",
       "      <td>10.966825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101995</th>\n",
       "      <td>-0.965163</td>\n",
       "      <td>-0.868066</td>\n",
       "      <td>0.687381</td>\n",
       "      <td>0.737635</td>\n",
       "      <td>0.201860</td>\n",
       "      <td>0.130245</td>\n",
       "      <td>-0.953401</td>\n",
       "      <td>0.992092</td>\n",
       "      <td>10.001159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101996</th>\n",
       "      <td>0.801107</td>\n",
       "      <td>0.875722</td>\n",
       "      <td>-0.990787</td>\n",
       "      <td>0.911076</td>\n",
       "      <td>0.353634</td>\n",
       "      <td>-0.470982</td>\n",
       "      <td>-0.985444</td>\n",
       "      <td>-0.509583</td>\n",
       "      <td>9.140565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101997</th>\n",
       "      <td>0.126903</td>\n",
       "      <td>-0.399359</td>\n",
       "      <td>-0.606135</td>\n",
       "      <td>-0.742466</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.924128</td>\n",
       "      <td>-0.592043</td>\n",
       "      <td>0.104867</td>\n",
       "      <td>9.935730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101998</th>\n",
       "      <td>-0.102098</td>\n",
       "      <td>-0.232576</td>\n",
       "      <td>-0.926793</td>\n",
       "      <td>-0.189997</td>\n",
       "      <td>-0.359941</td>\n",
       "      <td>-0.060435</td>\n",
       "      <td>-0.958450</td>\n",
       "      <td>0.175319</td>\n",
       "      <td>10.154265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101999</th>\n",
       "      <td>-0.393309</td>\n",
       "      <td>0.957055</td>\n",
       "      <td>-0.376406</td>\n",
       "      <td>-0.531251</td>\n",
       "      <td>-0.746653</td>\n",
       "      <td>-0.542937</td>\n",
       "      <td>0.639591</td>\n",
       "      <td>0.487487</td>\n",
       "      <td>10.207424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X_1       X_2       X_3       X_4       X_5       X_6       X_7  \\\n",
       "0      -0.165956  0.440649 -0.999771 -0.395335 -0.706488 -0.815323 -0.627480   \n",
       "1      -0.206465  0.077633 -0.161611  0.370439 -0.591096  0.756235 -0.945225   \n",
       "2      -0.165390  0.117380 -0.719226 -0.603797  0.601489  0.936523 -0.373152   \n",
       "3       0.752778  0.789213 -0.829912 -0.921890 -0.660339  0.756285 -0.803306   \n",
       "4       0.915779  0.066331  0.383754 -0.368969  0.373002  0.669251 -0.963423   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "101995 -0.965163 -0.868066  0.687381  0.737635  0.201860  0.130245 -0.953401   \n",
       "101996  0.801107  0.875722 -0.990787  0.911076  0.353634 -0.470982 -0.985444   \n",
       "101997  0.126903 -0.399359 -0.606135 -0.742466 -0.001000 -0.924128 -0.592043   \n",
       "101998 -0.102098 -0.232576 -0.926793 -0.189997 -0.359941 -0.060435 -0.958450   \n",
       "101999 -0.393309  0.957055 -0.376406 -0.531251 -0.746653 -0.542937  0.639591   \n",
       "\n",
       "             X_8          Y  \n",
       "0      -0.308879  10.218395  \n",
       "1       0.340935   9.505227  \n",
       "2       0.384645  10.492027  \n",
       "3      -0.157785   9.026440  \n",
       "4       0.500289  10.966825  \n",
       "...          ...        ...  \n",
       "101995  0.992092  10.001159  \n",
       "101996 -0.509583   9.140565  \n",
       "101997  0.104867   9.935730  \n",
       "101998  0.175319  10.154265  \n",
       "101999  0.487487  10.207424  \n",
       "\n",
       "[102000 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000 # Number of samples\n",
    "m = 1000 # Number of samples for validation/testing dataset\n",
    "d = 8 # Dimension of feature variable vector\n",
    "sigma = 0.01\n",
    "\n",
    "np.random.seed(1) # Set seed for reproducibility\n",
    "X = np.random.uniform(-1,1,(n + 2*m,d)) # 8 dimensional random vector X uniformly distributed on [-1,1]\n",
    "eps = np.random.normal(0,sigma,n + 2*m) # Gaussian noise\n",
    "\n",
    "Y = 8 + X[:,0]**2 + X[:,1]*X[:,2] + np.cos(X[:,3]) + np.exp(X[:,4]*X[:,5]) + 0.1*X[:,6] + eps # Output computation\n",
    "Y = Y.reshape((-1,1)) # Transpose output vector\n",
    "\n",
    "data = np.concatenate((X,Y), axis=1) # First 8 columns as input and last column as output\n",
    "data_df = pd.DataFrame(data) # Create dataframe of numpy matrix\n",
    "data_df.columns = ['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','Y']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "x_train = pd.DataFrame(data[:n,0:d]).to_numpy()\n",
    "y_train = pd.DataFrame(data[:n,d]).to_numpy()\n",
    "\n",
    "# Validation data\n",
    "\n",
    "x_val = pd.DataFrame(data[n:n + m,0:d]).to_numpy()\n",
    "y_val = pd.DataFrame(data[n:n + m,d]).to_numpy()\n",
    "\n",
    "# Testing data\n",
    "\n",
    "x_test = pd.DataFrame(data[n + m:n + 2*m,0:d]).to_numpy()\n",
    "y_test = pd.DataFrame(data[n + m:n + 2*m,d]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 7.6113 - val_loss: 0.3413\n",
      "Epoch 2/150\n",
      "3125/3125 [==============================] - 2s 532us/step - loss: 0.3051 - val_loss: 0.2597\n",
      "Epoch 3/150\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 0.2271 - val_loss: 0.1951\n",
      "Epoch 4/150\n",
      "3125/3125 [==============================] - 2s 529us/step - loss: 0.1878 - val_loss: 0.1573\n",
      "Epoch 5/150\n",
      "3125/3125 [==============================] - 2s 530us/step - loss: 0.1115 - val_loss: 0.0622\n",
      "Epoch 6/150\n",
      "3125/3125 [==============================] - 2s 527us/step - loss: 0.0489 - val_loss: 0.0391\n",
      "Epoch 7/150\n",
      "3125/3125 [==============================] - 2s 527us/step - loss: 0.0284 - val_loss: 0.0191\n",
      "Epoch 8/150\n",
      "3125/3125 [==============================] - 2s 529us/step - loss: 0.0174 - val_loss: 0.0139\n",
      "Epoch 9/150\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 0.0131 - val_loss: 0.0120\n",
      "Epoch 10/150\n",
      "3125/3125 [==============================] - 2s 582us/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 11/150\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 0.0086 - val_loss: 0.0078\n",
      "Epoch 12/150\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 0.0071 - val_loss: 0.0062\n",
      "Epoch 13/150\n",
      "3125/3125 [==============================] - 2s 553us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 14/150\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 15/150\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 16/150\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 17/150\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 18/150\n",
      "3125/3125 [==============================] - 2s 530us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 19/150\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 20/150\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 21/150\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 22/150\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 23/150\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 24/150\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 25/150\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 26/150\n",
      "3125/3125 [==============================] - 2s 506us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 27/150\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 28/150\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 29/150\n",
      "3125/3125 [==============================] - 2s 563us/step - loss: 9.3947e-04 - val_loss: 9.1564e-04\n",
      "Epoch 30/150\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 7.2686e-04 - val_loss: 6.7444e-04\n",
      "Epoch 31/150\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 5.7021e-04 - val_loss: 5.0666e-04\n",
      "Epoch 32/150\n",
      "3125/3125 [==============================] - 2s 553us/step - loss: 4.6370e-04 - val_loss: 4.3150e-04\n",
      "Epoch 33/150\n",
      "3125/3125 [==============================] - 2s 526us/step - loss: 3.9157e-04 - val_loss: 3.6602e-04\n",
      "Epoch 34/150\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 3.4964e-04 - val_loss: 3.4994e-04\n",
      "Epoch 35/150\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 3.1691e-04 - val_loss: 3.4436e-04\n",
      "Epoch 36/150\n",
      "3125/3125 [==============================] - 2s 524us/step - loss: 2.9522e-04 - val_loss: 2.8899e-04\n",
      "Epoch 37/150\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.8119e-04 - val_loss: 2.5704e-04\n",
      "Epoch 38/150\n",
      "3125/3125 [==============================] - 2s 526us/step - loss: 2.7230e-04 - val_loss: 3.0262e-04\n",
      "Epoch 39/150\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 2.6533e-04 - val_loss: 2.5581e-04\n",
      "Epoch 40/150\n",
      "3125/3125 [==============================] - 2s 530us/step - loss: 2.5719e-04 - val_loss: 2.4842e-04\n",
      "Epoch 41/150\n",
      "3125/3125 [==============================] - 2s 524us/step - loss: 2.5411e-04 - val_loss: 2.3684e-04\n",
      "Epoch 42/150\n",
      "3125/3125 [==============================] - 2s 522us/step - loss: 2.5121e-04 - val_loss: 2.4794e-04\n",
      "Epoch 43/150\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 2.5202e-04 - val_loss: 2.5030e-04\n",
      "Epoch 44/150\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 2.4510e-04 - val_loss: 2.3244e-04\n",
      "Epoch 45/150\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 2.4224e-04 - val_loss: 2.2300e-04\n",
      "Epoch 46/150\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 2.4117e-04 - val_loss: 2.2052e-04\n",
      "Epoch 47/150\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 2.4137e-04 - val_loss: 2.3708e-04\n",
      "Epoch 48/150\n",
      "3125/3125 [==============================] - 2s 553us/step - loss: 2.3826e-04 - val_loss: 2.1867e-04\n",
      "Epoch 49/150\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 2.4002e-04 - val_loss: 2.8239e-04\n",
      "Epoch 50/150\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 2.3672e-04 - val_loss: 2.1212e-04\n",
      "Epoch 51/150\n",
      "3125/3125 [==============================] - 2s 553us/step - loss: 2.3465e-04 - val_loss: 2.3535e-04\n",
      "Epoch 52/150\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 2.3404e-04 - val_loss: 2.1666e-04\n",
      "Epoch 53/150\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 2.3450e-04 - val_loss: 2.6269e-04\n",
      "Epoch 54/150\n",
      "3125/3125 [==============================] - 2s 519us/step - loss: 2.3320e-04 - val_loss: 2.1299e-04\n",
      "Epoch 55/150\n",
      "3125/3125 [==============================] - 2s 521us/step - loss: 2.3352e-04 - val_loss: 2.1917e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd386332370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1) # Set seed for reproducibility\n",
    "\n",
    "# Neural network model\n",
    "\n",
    "neurons = 25 # 25 neurons in the hidden layer\n",
    "\n",
    "NN_model = tf.keras.models.Sequential()\n",
    "NN_model.add(tf.keras.layers.Dense(neurons, activation=tf.nn.sigmoid, input_dim = d)) \n",
    "NN_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "NN_model.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "# Training process\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=10**(-5), patience=5) # Early stopping criterion\n",
    "\n",
    "NN_model.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_val, y_val), callbacks=[callback], verbose=1) # Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
